{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "The following code was designed in order to track the location of a single animal across the course of a single video session.  After initally loading in the video, the user is able to crop the video frame using a rectangle selection tool.  A background reference frame is then specified, either by taking a median of several frames in the video, or by the user providing a short video of the same environment without an animal in it.  By comparing each frame in the video to the reference frame, the location of the animal can be tracked.  It is imperative that the reference frame of the video is not shifted from the actual video.  For this reason, if a separate video is supplied, it is best that it be acquired on the same day as the behavioral recording.  The animal's center of mass, in x,y coordinates, is then recorded, as well as the distance in pixels that the animal moves from one frame to the next. Lastly, the user can specify regions of interest in the frame (e.g. left, right) using a polygon drawing tool and record for each frame whether the animal is in the region of interest.  Options for summarizing the data are also provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Load Necessary Packages\n",
    "The following code loads neccessary packages and need not be changed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import LocationTracking_Functions as lt\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import matplotlib\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. User Sets Directory and File Information, and Specifies ROI Names, if any.\n",
    "`dpath` : The directory path of the folder containing the video to be processed. Note that if you are using a Windows path with backslashes, place an ‘r’ in front of the directory path to avoid an error (e.g., r’\\Users\\DeniseCaiLab\\Videos’).\n",
    "\n",
    "`file` : The filename of the video, including the file extension.\n",
    "\n",
    "`start` : The frame of the video on which to begin processing.  0 is the first frame. \n",
    "\n",
    "`end` : The frame of the video on which to end processing.  If the user would like to process from the start frame to the end of the video, this can be set to None.\n",
    "\n",
    "`region_names` : If the user would like to measure the time spent in ROIs, a list containing the names of the ROIs should be provided.  A Python list is defined by a set of square brackets, and each ROI name should be placed in quotations, separated by a comma.  If no ROIs are to be defined, this can be set to None (i.e., `‘region_names’ : None`).\n",
    "\n",
    "`dsmpl` : The amount to down-sample each frame. A value of 1 indicates no down-sampling, while a value of 0.25 indicates that each frame will be down-sampled to ¼ its original size.  Note that if down-sampling is performed, all pixel coordinate output will be in the dimensions of the down-sampled video.\n",
    "\n",
    "`stretch` : Allows the user to alter the aspect ratio of the presented output.  This is useful when videos have irregular dimensions and are difficult to see (e.g., an aspect ratio of 1:100).  The width/height will be scaled by the factor provided. Note that this only affects the appearance of visualizations and does not modify the video or the interpretation of the output.\n",
    "\n",
    "***Processing going slow?  Consider downsampling!***  Often times tracking does not not require 1080p or whatever high def resolution videos are sometimes acquired using. Try setting `dsmpl` to something lower than 1 to implement downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Users/hcjiang/Dropbox (UMass Medical School)/olfactory behavior/habituation\"\n",
    "output_dir = \"/Users/hcjiang/Desktop/ezTrack/ezTrack output/\" \n",
    "video_name = '20220207_mouae6_cd20 ko_female15w_1-hab.mp4'\n",
    "prefix = re.sub('.mp4', '',video_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"location_raw\",\"location_trace\",\"heatmap\",\"location_trace_central_distance\",\"distance_across_session\",\"summary\"]\n",
    "output_path = []\n",
    "for i in folders:\n",
    "    newpath = output_dir + \"/\" +i\n",
    "    output_path.append(newpath)\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = {\n",
    "    'dpath'        : input_dir,  \n",
    "    'file'         : video_name,\n",
    "    'start'        : 0, \n",
    "    'end'          : 45553, \n",
    "    'region_names' : ['avoidance zone','odorized zone','odor'],\n",
    "    'dsmpl'        : 0.25,\n",
    "    'stretch'      : dict(width=1, height=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Load Video and Crop Frame if Desired\n",
    "To crop video frame, after running code below, select the box selection tool below the image (square with a plus sign).  To start drawing region to be included in the analyis, double click image.  Double click again to finalize region.  If you decide to change region, it is best to rerun this cell and subsequent steps.\n",
    "\n",
    "If the size of the image is too small/large, alter the first line of code.  100 is the standard size.  200 will produce an image 2x the size, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%output size = 200\n",
    "\n",
    "img_crp, video_dict = lt.LoadAndCrop(video_dict, cropmethod='Box')\n",
    "img_crp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. (Optional) Mask Internal Regions\n",
    "\n",
    "The following code is used to exclude internal portion/s of the field of view from the analysis. After running cell below, draw regions to be excluded.  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 200\n",
    "\n",
    "img_mask, video_dict['mask'] = lt.Mask_select(video_dict)\n",
    "img_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Define Reference Frame for Location Tracking.\n",
    "For location tracking to work, a view of box without animal must be provided.  Below there are two ways to do this.  **Option 1** provides a method to remove the animal from the video.  This option works well provided the animal doesn't stay in the same location for >50% of the session. Alternatively, with **Option 2**, the user can simply define a video file in the same folder that doesn't have in animal in it.  Option 1 is generally recommended, being simpler to obtain. \n",
    "\n",
    "The number of random frames used can be changed with the variable `num_frames`.  Alternatively, if the user would like to manually define the frames to be used, `frames` can be set to a list frame numbers: e.g. `frames = np.arange(100,500,5)`, which would select every 5th frame in the range 100-500).  Setting `frames` will override the num_frames variable selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Create reference frame by removing animal from video\n",
    "The following code takes a random sample of frames across the session and creates an average of them by taking the median for each pixel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 200\n",
    "\n",
    "video_dict['reference'], img_ref = lt.Reference(video_dict, num_frames=50, frames=None) \n",
    "img_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - User specifies video of empty box\n",
    "The following code allows the user to specify a different file.  Set video_dict['altfile'] to the alternative filename.  \n",
    "\n",
    "Defining `frames` is necessary if the alternative video is a different length than the reference video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 100\n",
    "\n",
    "video_dict['altfile'] = 'EmptyBox.avi' \n",
    "\n",
    "video_dict['reference'], img_ref = lt.Reference(video_dict, num_frames=50, altfile=True, frames=[0]) \n",
    "img_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. (Optional) Use Interactive Plot to Define Regions of Interest.  \n",
    "\n",
    "After running cell below, draw regions of interest on presented image in the order you provided them (in Cell 2).  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell.\n",
    "\n",
    "***Note*** that there are no problems if regions overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 200\n",
    "\n",
    "\n",
    "img_roi, video_dict['roi_stream'] = lt.ROI_plot(video_dict)\n",
    "img_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. (Optional) Define Scale for Distance Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Select two points of known distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running cell below, click on any two points and the distance between them, in pixel units, will be presented. Will be used to convert pixel distance to other scale. Note that once drawn, points can be dragged or you can click again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 200\n",
    "\n",
    "img_scl, video_dict['scale'] = lt.DistanceTool(video_dict)\n",
    "img_scl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Define real-world distance between points\n",
    "Below, define the distance between the points selected, and the scale. Note that scale can be any desired text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 10\n",
    "scale = 'cm'\n",
    "\n",
    "video_dict['scale'] = lt.setScale(distance, scale, video_dict['scale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Track Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Set Location Tracking Parameters\n",
    "Location tracking examines the deviance of each frame in a video from the reference frame on a pixel by pixel basis.  For each frame it calculates the center of mass for these differences (COM) to define the center of the animal.  \n",
    "\n",
    "`loc_thresh` : This parameter represents a percentile threshold and can take on values between 0-100.  Each frame is compared to the reference frame.  Then, to remove the influence of small fluctuations, any differences below a given percentile (relative to the maximum difference) are set to 0.  We use a value of 99.5 with good success.\n",
    "\n",
    "`use_window` : This parameter is incredibly helpful if objects other than the animal temporarily enter the field of view during tracking (such as an experimenter’s hand manually delivering a stimulus or reward).  When use_window is set to True, a square window with the animal's position on the prior frame at its center is given more weight when searching for the animal’s location (because an animal presumably can't move far from one frame to the next).  In this way, the influence of objects entering the field of view can be avoided.  If use_window is set to True, the user should consider window_size and window_weight.\n",
    "\n",
    "`window_size` : This parameter only impacts tracking when use_window is set to True.  This defines the size of the square window surrounding the animal that will be more heavily weighted in pixel units.  We typically set this to 2-3 times the animal’s size (if an animal is 100 pixels at its longest, we will set window_size to 200).  Note that to determine the size of the animal in pixels, the user can reference any image of the arena presented in ezTrack, which has the pixel coordinate scale on its axes.\n",
    "\n",
    "`window_weight` : This parameter only impacts tracking when use_window is set to True.  When window_weight is set to 1, pixels outside of the window are not considered at all; at 0, they are given equal weight. Notably, setting a high value that is still not equal to 1 (e.g., 0.9) should allow ezTrack to more rapidly find the animal if, by chance, it moves out of the window.  \n",
    "\n",
    "`method` : This parameter determines the luminosity of the object ezTrack will search for relative to the background and accepts values of 'abs', 'light', and 'dark'. Option 'abs' does not take into consideration whether the animal is lighter or darker than the background and will therefore track the animal across a wide range of backgrounds. 'light' assumes the animal is lighter than the background, and 'dark' assumes the animal is darker than the background. Option 'abs' generally works well, but there are situations in which you may wish to use the others.  For example, if a tether is being used that is opposite in color to the animal (a white wire and a black mouse), the ‘abs’ method is much more likely to be biased by the wire, whereas option ‘dark’ will look for the darker mouse.\n",
    "\n",
    "`rmv_wire` : When rmv_wire is set to True, an algorithm is used to attempt to remove wires from the field of view.  If rmv_wire is set to True, the user should consider wire_krn.\n",
    "\n",
    "`wire_krn` : This parameter only impacts tracking when rmv_wire is set to True. This value should be set between the width of the wire and the width of the animal, in pixel units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_params = {\n",
    "    'loc_thresh'    : 99.5, \n",
    "    'use_window'    : True, \n",
    "    'window_size'   : 100, \n",
    "    'window_weight' : .9, \n",
    "    'method'        : 'abs',\n",
    "    'rmv_wire'      : False, \n",
    "    'wire_krn'      : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. (Optional) Display Examples of Location Tracking to Confirm Threshold\n",
    "In order to confirm threshold is working, a subset of images is analyzed and displayed using the selected `tracking_params`.  The original image is displayed on the left and the difference values to the right.  The center of mass (COM) is pinpointed on images.  Notably, because individual frames are used, window settings are not applicable here.  Because of this, actual tracking in video is likely to be better.\n",
    "\n",
    "The user can change the number examples below as they see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 100\n",
    "\n",
    "img_exmpls = lt.LocationThresh_View(video_dict, tracking_params, examples=4)\n",
    "img_exmpls.cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Track Location and Save Results to .csv File\n",
    "For each frame the location of the animal's center of mass is recorded in x/y coordinates.  If ROIs are supplied, for each frame it is determined whether the animal is in each of the ROIs.  Frame-by-frame distance is also calculated in pixel units.  This data is returned in a Pandas dataframe with columns: frame, x, y, dist, and whether the animal is in each ROI specified (True/False).  Data is saved to a .csv in the same folder as the video.  First 5 rows of data are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = lt.TrackLocation(video_dict, tracking_params)   \n",
    "location.to_csv(output_path[0] + \"/\" + prefix +'_locationOutput.csv', index=False)\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8d. (Optional) Display Animal Distance/Location Across Session\n",
    "Below, the animals distance and location across the video is plotted.  Smooth traces are expected in the case where the animal is tracked consistently across the session.  Under heatmap, sigma controls 'binning' of location. When 'sigma = None' default value is provided; but sigma can also be set to any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 200\n",
    "\n",
    "w, h = 1000,300 \n",
    "\n",
    "plt_dist = hv.Curve((location['Frame'],location['Distance_px']),'Frame','Pixel Distance').opts(\n",
    "    height=h,width=w,color='red',title=\"Distance Across Session\",toolbar=\"below\")\n",
    "plt_trks = lt.showtrace(video_dict, location, color=\"red\", alpha=.05, size=5)\n",
    "plt_hmap = lt.Heatmap(video_dict, location, sigma=5)\n",
    "\n",
    "(plt_trks + plt_hmap + plt_dist).cols(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for poly in range(len(video_dict['roi_stream'].data['xs'])):\n",
    "            x = np.array(video_dict['roi_stream'].data['xs'][poly]) #x coordinates\n",
    "            y = np.array(video_dict['roi_stream'].data['ys'][poly]) #y coordinates\n",
    "            lst.append( [ (x[vert],y[vert]) for vert in range(len(x)) ] )\n",
    "\n",
    "patches = []\n",
    "num_polygons = len(lst)\n",
    "\n",
    "\n",
    "for i in range(num_polygons):\n",
    "    polygon = Polygon(lst[i], True, facecolor = \"lightblue\", alpha = 0.2)\n",
    "    polyEdge = Polygon(lst[i], fill=False, alpha=1 ,edgecolor= \"black\",linewidth=1, linestyle = '--')\n",
    "    patches.append(polygon)\n",
    "    patches.append(polyEdge)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "imgplot = plt.imshow(video_dict['reference'],cmap=\"gray\")\n",
    "scalebar = ScaleBar(video_dict['scale'][\"factor\"], units=\"cm\", color=\"white\",box_alpha=0, location=\"lower right\",font_properties={'size' : 20})\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "p = PatchCollection(patches, match_original=True)\n",
    "plt.scatter(location['X'],location['Y'], color = \"Yellow\", alpha =0.05)\n",
    "plt.title(\"Motion trace\", fontsize=40)\n",
    "ax.add_collection(p)\n",
    "ax.autoscale()\n",
    "plt.gcf().set_size_inches(15, 8)\n",
    "plt.gca().add_artist(scalebar)\n",
    "plt.savefig(output_path[1] + \"/\" + prefix +'_Motion_trace.pdf', bbox_inches='tight' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = []\n",
    "num_polygons = len(lst)\n",
    "\n",
    "\n",
    "for i in range(num_polygons):\n",
    "    polygon = Polygon(lst[i], True, facecolor = \"white\", alpha = 0)\n",
    "    polyEdge = Polygon(lst[i], fill=False, alpha=1 ,edgecolor= \"white\",linewidth=2, linestyle = '--')\n",
    "    patches.append(polygon)\n",
    "    patches.append(polyEdge)\n",
    "\n",
    "\n",
    "heatmap = np.zeros(video_dict['reference'].shape)\n",
    "for frame in range(len(location)):\n",
    "    Y,X = int(location.Y[frame]), int(location.X[frame])\n",
    "    heatmap[Y,X]+=1\n",
    "    \n",
    "sigma = 5 # change the number for heatmap resolusion \n",
    "heatmap = cv2.GaussianBlur(heatmap,(0,0),sigma)\n",
    "heatmap = (heatmap / heatmap.max())*255\n",
    "\n",
    "scalebar = ScaleBar(video_dict['scale'][\"factor\"], units=\"cm\", color=\"white\",box_alpha=0, location=\"lower right\",font_properties={'size' : 15})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "p = PatchCollection(patches, match_original=True)\n",
    "plt.imshow(heatmap, cmap=\"gnuplot2\") # change color, google \"matplotlib cmap\" for details\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap\", fontsize = 40)\n",
    "plt.gca().add_artist(scalebar)\n",
    "ax.add_collection(p)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.gcf().set_size_inches(15, 8)\n",
    "plt.savefig(output_path[2] + \"/\" + prefix +'_Heatmap.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for poly in range(len(video_dict['roi_stream'].data['xs'])):\n",
    "    x = np.array(video_dict['roi_stream'].data['xs'][poly]) #x coordinates\n",
    "    y = np.array(video_dict['roi_stream'].data['ys'][poly]) #y coordinates\n",
    "    lst.append( [ (x[vert],y[vert]) for vert in range(len(x)) ] )\n",
    "poly = hv.Polygons(lst).opts(fill_alpha=0.1,line_dash='dashed')\n",
    "\n",
    "p1 = np.average(lst[2], axis=0) # center of sencond ROI, if petri dish is the thrid ROI you circled, please make the lst[2]\n",
    "all_coord = np.array([location['X'],location['Y']]).T\n",
    "p2 = np.average(all_coord, axis=0) # averge position\n",
    "pall = np.array((p1, p2))\n",
    "dist = str(round(math.dist(p1, p2)*video_dict['scale'][\"factor\"], 2)) # distance\n",
    "patches = []\n",
    "num_polygons = len(lst)\n",
    "\n",
    "\n",
    "for i in range(num_polygons):\n",
    "    polygon = Polygon(lst[i], True, facecolor = \"lightblue\", alpha = 0.2)\n",
    "    polyEdge = Polygon(lst[i], fill=False, alpha=1 ,edgecolor= \"black\",linewidth=1, linestyle = '--')\n",
    "    patches.append(polygon)\n",
    "    patches.append(polyEdge)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "imgplot = plt.imshow(video_dict['reference'],cmap=\"gray\")\n",
    "scalebar = ScaleBar(video_dict['scale'][\"factor\"], units=\"cm\", color=\"white\",box_alpha=0, location=\"lower right\",font_properties={'size' : 20})\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "p = PatchCollection(patches, match_original=True)\n",
    "plt.scatter(location['X'],location['Y'], color = \"Yellow\", alpha =0.05)\n",
    "plt.scatter(p1[0],p1[1], color = \"Blue\", alpha =1, s=150)\n",
    "plt.scatter(p2[0],p2[1], color = \"Red\", alpha =1, s=150)\n",
    "#plt.scatter([p1[0] for p1 in pall],[p1[1] for p1 in pall],color = \"Red\", alpha =1, s =150)\n",
    "plt.title(\"Distance between average location \\n to odorant = \"+dist + \"cm\", fontsize = 30)\n",
    "ax.add_collection(p)\n",
    "ax.autoscale()\n",
    "plt.gcf().set_size_inches(15, 8)\n",
    "plt.gca().add_artist(scalebar)\n",
    "plt.savefig(output_path[3] + \"/\" + prefix +'_Motion_trace_distance.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(location['Frame'],location['Distance_px']*video_dict['scale'][\"factor\"],color=\"red\")\n",
    "plt.title(\"Distance Across Session\", fontsize = 40)\n",
    "plt.xlabel(\"frame\", fontsize = 30)\n",
    "plt.ylabel(\"cm\", fontsize = 30)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.gcf().set_size_inches(15, 8)\n",
    "plt.savefig(output_path[4] + \"/\" + prefix +'_distance_across_session.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. (Optional) Create Binned Summary Report and Save\n",
    "The code below allows the user to either save a csv containing summary data for user-defined bins (e.g. proportion of time in each region and distance travelled for each minute) or a session-wide average. \n",
    "\n",
    "***If you only want a session avg***, set `bin_dict = None`\n",
    "\n",
    "***To specify bins***, set bin_dict using the following notation, where start and stop represent time in frames:\n",
    "\n",
    "```\n",
    "bin_dict = {\n",
    "    'BinName1': (start, stop),\n",
    "    'BinName2': (start, stop),\n",
    "    'BinName3': (start, stop),\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dict = None\n",
    "\n",
    "\n",
    "\n",
    "summary = lt.Summarize_Location(location, video_dict, bin_dict=bin_dict)\n",
    "summary['Average_position_to_dish (cm)'] = dist\n",
    "summary.to_csv(output_path[5] + \"/\" + prefix + '_SummaryStats.csv', index=False)\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. (Optional) View Video of Tracking\n",
    "**Note** that tracking must be done before this (Step 7c). \n",
    "\n",
    "`start` : The frame video playback is to be started on. Note that this is relative to the start of tracking, where 0 is the first tracked frame.\n",
    "\n",
    "`stop` : The frame video playback is to end on.  Note that this is relative to the start of tracking, where 0 is the first tracked frame.\n",
    "\n",
    "`fps` : The speed of video playback.  Must be an integer.  Video playback may also be slower depending upon computer speed. \n",
    "\n",
    "`resize` : If the user wants the output to be larger or smaller, or they want the aspect ratio to be different, resize can be supplied as in the following example:\n",
    "\t`‘resize’ : (100,200)`\n",
    "Here, the first number corresponds to the adjusted width of the frame, whereas the second number corresponds to the adjusted height.  Both numbers reflect pixel units and should be integers. Set resize equal to None if no resizing is to be done.\n",
    "\n",
    "`save_video` : To save the video clip, set to True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dict = {\n",
    "    'start'      : 0, \n",
    "    'stop'       : 1000, \n",
    "    'fps'        : 100,\n",
    "    'resize'     : None,\n",
    "    'save_video' : False\n",
    "}\n",
    "\n",
    "lt.PlayVideo(video_dict,display_dict,location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
